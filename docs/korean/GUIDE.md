# Universal Document Translator - ì™„ì „ ê°€ì´ë“œ

**ì´ˆê¸‰ë¶€í„° ì „ë¬¸ê°€ê¹Œì§€: OCR ë° AI ë¬¸ì„œ ë²ˆì—­ ì´í•´í•˜ê¸°**

---

## ëª©ì°¨

1. [ì´ˆë“±í•™ìƒìš© (8-12ì„¸)](#ì´ˆë“±í•™ìƒìš©-8-12ì„¸)
2. [ì¤‘ê³ ë“±í•™ìƒìš© (13-18ì„¸)](#ì¤‘ê³ ë“±í•™ìƒìš©-13-18ì„¸)
3. [ëŒ€í•™ìƒ ë° ì´ˆë³´ììš©](#ëŒ€í•™ìƒ-ë°-ì´ˆë³´ììš©)
4. [ê°œë°œììš©](#ê°œë°œììš©)
5. [ì „ë¬¸ê°€ ë° ê³ ê¸‰ ì‚¬ìš©ììš©](#ì „ë¬¸ê°€-ë°-ê³ ê¸‰-ì‚¬ìš©ììš©)

---

## ì´ˆë“±í•™ìƒìš© (8-12ì„¸)

### ì´ í”„ë¡œì íŠ¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?

í•œêµ­ì—ì„œ íœ´ê°€ë¥¼ ë³´ë‚´ëŠ”ë° í•œêµ­ì–´ë¡œ ì‘ì„±ëœ ë©”ë‰´íŒì„ ë³¸ë‹¤ê³  ìƒìƒí•´ë³´ì„¸ìš”. ìŒì‹ì´ ë¬´ì—‡ì¸ì§€ ëª¨ë¥´ê² ì£ ! ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì€ **ë§ˆë²•ì˜ ì¹´ë©”ë¼**ì™€ ê°™ìŠµë‹ˆë‹¤:

1. ğŸ“¸ ë©”ë‰´íŒ ì‚¬ì§„ ì´¬ì˜
2. ğŸ‘€ ëª¨ë“  í•œêµ­ì–´ ë‹¨ì–´ ì½ê¸° (ì»´í“¨í„°ì¸ë°ë„!)
3. ğŸ—£ï¸ ëª¨ë“  ê²ƒì„ ì˜ì–´ë¡œ ë²ˆì—­
4. ğŸ’¡ ê° ìš”ë¦¬ê°€ ë¬´ì—‡ì¸ì§€ ì•Œë ¤ì£¼ê³  ì„ íƒì„ ë„ì™€ì¤Œ!

### ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?

3ë‹¨ê³„ ê²Œì„ì²˜ëŸ¼ ìƒê°í•˜ì„¸ìš”:

**1ë‹¨ê³„: ì‚¬ì§„ ì°ê¸°**
- ì»´í“¨í„°ì—ì„œ ì›¹ì‚¬ì´íŠ¸ ì—´ê¸°
- í° ì—…ë¡œë“œ ë²„íŠ¼ í´ë¦­
- ê¸€ìê°€ ìˆëŠ” ì¢…ì´(ë©”ë‰´, í‘œì§€íŒ, ë©”ëª¨ ë“±) ì‚¬ì§„ ì°ê¸°

**2ë‹¨ê³„: ë§ˆë²•ì˜ ì»´í“¨í„° ì½ê¸°**
- ì»´í“¨í„°ê°€ ì‚¬ì§„ì„ ë´„
- ëª¨ë“  ë‹¨ì–´ë¥¼ ì°¾ìŒ (ì´ê²ƒì„ "OCR"ì´ë¼ê³  í•¨ - ë¡œë´‡ ëˆˆ ê°™ì€ ê²ƒ!)
- ë‹¨ì–´ê°€ ì–´ë–¤ ì–¸ì–´ì¸ì§€ ì•Œì•„ëƒ„

**3ë‹¨ê³„: ë²ˆì—­ ë°›ê¸°**
- ì»´í“¨í„°ê°€ AIë¥¼ ì‚¬ìš© (ë§¤ìš° ë˜‘ë˜‘í•œ ë¡œë´‡ ë‘ë‡Œì²˜ëŸ¼)
- ëª¨ë“  ë‹¨ì–´ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­
- ê°€ê²©, ë‚ ì§œ, ê²½ê³  ê°™ì€ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì•Œë ¤ì¤Œ

### í•  ìˆ˜ ìˆëŠ” ë©‹ì§„ ì¼ë“¤

- ë©”ë‰´ë¥¼ ì½ê³  ìŒì‹ì´ ë¬´ì—‡ì¸ì§€ ì•Œë ¤ì¤Œ
- ì—¬í–‰í•  ë•Œ ê±°ë¦¬ í‘œì§€íŒ ë²ˆì—­
- ì•½ë³‘ì„ ì½ê³  ì‚¬ìš©ë²• ì•Œë ¤ì¤Œ
- ì˜ìˆ˜ì¦ì„ êµ¬ë§¤ ëª©ë¡ìœ¼ë¡œ ë³€í™˜
- ë‹¤ë¥¸ ì–¸ì–´ë¡œ ëœ ìˆ™ì œ ì„¤ëª…

### ì§ì ‘ í•´ë³´ê¸°!

1. ì»´í“¨í„°ì—ì„œ `http://localhost:3000` ì ‘ì†
2. í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ì‚¬ì§„ì„ ì›¹ì‚¬ì´íŠ¸ì— ë“œë˜ê·¸
3. "ë¬¸ì„œ ë¶„ì„" í´ë¦­
4. ëª‡ ì´ˆ ê¸°ë‹¤ë¦¼
5. ë²ˆì—­ê³¼ ì„¤ëª… ë³´ê¸°!

**ì˜ˆì‹œ:**
- ì¼ë³¸ ë©”ë‰´ ì‚¬ì§„ ì—…ë¡œë“œ
- ëª©í‘œ ì–¸ì–´ë¡œ "ì˜ì–´" ì„ íƒ
- ê²°ê³¼: "ë¼ë©˜ - ë¼ì§€ê³ ê¸°, ê³„ë€, ì•¼ì±„ê°€ ë“¤ì–´ê°„ êµ­ìˆ˜ ìˆ˜í”„"

---

## ì¤‘ê³ ë“±í•™ìƒìš© (13-18ì„¸)

### í”„ë¡œì íŠ¸ ê°œìš”

ë‹¤ìŒì„ ê²°í•©í•œ **AI ê¸°ë°˜ ë¬¸ì„œ ë¶„ì„ ì‹œìŠ¤í…œ**ì…ë‹ˆë‹¤:
- **ì»´í“¨í„° ë¹„ì „** (ì»´í“¨í„°ê°€ ì´ë¯¸ì§€ë¥¼ "ë³´ëŠ”" ë°©ë²•)
- **ê´‘í•™ ë¬¸ì ì¸ì‹** (OCR - ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ)
- **ìì—°ì–´ ì²˜ë¦¬** (í…ìŠ¤íŠ¸ ì´í•´ ë° ë²ˆì—­)
- **ê¸°ê³„ í•™ìŠµ** (ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ë˜‘ë˜‘í•´ì§€ëŠ” AI)

### ë°°ìš¸ ìˆ˜ ìˆëŠ” ê²ƒ

ì´ í”„ë¡œì íŠ¸ë¥¼ ì´í•´í•˜ë©´ ë‹¤ìŒì„ ë°°ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
1. **ì´ë¯¸ì§€ ì²˜ë¦¬** - ì»´í“¨í„°ê°€ ì‚¬ì§„ì„ ë¶„ì„í•˜ëŠ” ë°©ë²•
2. **íŒ¨í„´ ì¸ì‹** - AIê°€ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ëŠ” ë°©ë²•
3. **ì–¸ì–´ ë²ˆì—­** - ì»´í“¨í„°ê°€ ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ì´í•´í•˜ëŠ” ë°©ë²•
4. **ì›¹ ê°œë°œ** - ì›¹ì‚¬ì´íŠ¸ì™€ ì•±ì´ ì‘ë™í•˜ëŠ” ë°©ë²•
5. **ë°ì´í„°ë² ì´ìŠ¤** - ì •ë³´ê°€ ì €ì¥ë˜ê³  ê²€ìƒ‰ë˜ëŠ” ë°©ë²•

### ì‘ë™ ì›ë¦¬ (ê¸°ìˆ  ê°œìš”)

```
ì´ë¯¸ì§€ â†’ ì´ë¯¸ì§€ ì²˜ë¦¬ â†’ í…ìŠ¤íŠ¸ ê°ì§€ â†’ í…ìŠ¤íŠ¸ ì¸ì‹ â†’
AI ë¶„ì„ â†’ ë²ˆì—­ â†’ êµ¬ì¡°í™”ëœ ê²°ê³¼
```

**ë‹¨ê³„ë³„ í”„ë¡œì„¸ìŠ¤:**

1. **ì—…ë¡œë“œ ë‹¨ê³„**
   - ì´ë¯¸ì§€ ì—…ë¡œë“œ (JPG, PNG ë˜ëŠ” PDF)
   - ì„œë²„ê°€ íŒŒì¼ì„ ë°›ê³  ê²€ì¦
   - ì´ë¯¸ì§€ê°€ ì„ì‹œ ì €ì¥ë¨

2. **ì „ì²˜ë¦¬**
   - ì»´í“¨í„°ê°€ ë°ê¸°ì™€ ëŒ€ë¹„ ì¡°ì •
   - ë…¸ì´ì¦ˆì™€ íë¦¼ ì œê±°
   - íšŒì „ëœ í…ìŠ¤íŠ¸ ì •ë ¬
   - OCRì„ ìœ„í•´ ì´ë¯¸ì§€ ì¤€ë¹„

3. **OCR (í…ìŠ¤íŠ¸ ì¶”ì¶œ)**
   - AIê°€ ì´ë¯¸ì§€ë¥¼ í”½ì…€ ë‹¨ìœ„ë¡œ ìŠ¤ìº”
   - í…ìŠ¤íŠ¸ì²˜ëŸ¼ ë³´ì´ëŠ” ì˜ì—­ ì°¾ê¸°
   - ê°œë³„ ë¬¸ì ì¸ì‹
   - ë¬¸ìë¥¼ ë‹¨ì–´ë¡œ ê²°í•©
   - ì¡´ì¬í•˜ëŠ” ì–¸ì–´ ê°ì§€

4. **AI ë¶„ì„**
   - Claude AIì— í…ìŠ¤íŠ¸ ì „ì†¡ (ChatGPTì™€ ìœ ì‚¬)
   - AIê°€ ë§¥ë½ê³¼ ì˜ë¯¸ ì´í•´
   - ë¬¸ì„œ ìœ í˜• ë¶„ë¥˜ (ë©”ë‰´, ì˜ìˆ˜ì¦, ê³„ì•½ì„œ ë“±)
   - ì¤‘ìš” ì •ë³´ ì¶”ì¶œ (ë‚ ì§œ, ê°€ê²©, ì´ë¦„)
   - ë²ˆì—­ ë° ì¡°ì–¸ ìƒì„±

5. **ê²°ê³¼ í‘œì‹œ**
   - ì‹ ë¢°ë„ ì ìˆ˜ì™€ í•¨ê»˜ ì›ë³¸ í…ìŠ¤íŠ¸ í‘œì‹œ
   - ì „ì²´ ë²ˆì—­ í‘œì‹œ
   - í•µì‹¬ ì •ë³´ ê°•ì¡°
   - ë¬¸ë§¥ì  ì¡°ì–¸ ì œê³µ

### ì‚¬ìš©ëœ ê¸°ìˆ 

- **ë°±ì—”ë“œ**: Python (í”„ë¡œê·¸ë˜ë° ì–¸ì–´) + FastAPI (ì›¹ í”„ë ˆì„ì›Œí¬)
- **í”„ë¡ íŠ¸ì—”ë“œ**: React (ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤) + Next.js (ì›¹ í”„ë ˆì„ì›Œí¬)
- **OCR**: EasyOCR (í…ìŠ¤íŠ¸ ì¸ì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬)
- **AI**: Claude API (ì–¸ì–´ ëª¨ë¸)
- **ë°ì´í„°ë² ì´ìŠ¤**: PostgreSQL (ë°ì´í„° ì €ì¥)
- **ìŠ¤íƒ€ì¼ë§**: Tailwind CSS (ë””ìì¸)

### í”„ë¡œì íŠ¸ ì‹¤í–‰

**ì‚¬ì „ ìš”êµ¬ì‚¬í•­:**
- Python 3.10+ ì„¤ì¹˜
- Node.js 18+ ì„¤ì¹˜
- Anthropicì—ì„œ Claude API í‚¤ ë°›ê¸°

**ë¹ ë¥¸ ì‹œì‘:**
```bash
# 1. í”„ë¡œì íŠ¸ í´ë¡ 
git clone https://github.com/dddoing/ocrproject.git
cd ocrproject

# 2. í™˜ê²½ ì„¤ì •
echo "ANTHROPIC_API_KEY=your_key" > .env

# 3. Dockerë¡œ ì‹¤í–‰
docker-compose up
```

**ìˆ˜ë™ ì„¤ì •:**
```bash
# ë°±ì—”ë“œ
cd backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --reload

# í”„ë¡ íŠ¸ì—”ë“œ (ìƒˆ í„°ë¯¸ë„)
cd frontend
npm install
npm run dev
```

### ì‹¤ì œ ì ìš© ì‚¬ë¡€

1. **ì—¬í–‰**: í‘œì§€íŒ, ë©”ë‰´, í‹°ì¼“ ë²ˆì—­
2. **ë¹„ì¦ˆë‹ˆìŠ¤**: ê³„ì•½ì„œ, ì²­êµ¬ì„œ, ì˜ìˆ˜ì¦ ë¶„ì„
3. **êµìœ¡**: í•™ìŠµ ìë£Œ, ì—°êµ¬ ë…¼ë¬¸ ë²ˆì—­
4. **ì˜ë£Œ**: ì²˜ë°©ì „, ì˜ë£Œ ì§€ì¹¨ ì½ê¸°
5. **ì‡¼í•‘**: ì œí’ˆ ë¼ë²¨, ê°€ê²© ì´í•´

---

## ëŒ€í•™ìƒ ë° ì´ˆë³´ììš©

### ê¸°ìˆ  ì•„í‚¤í…ì²˜

**ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì§€í–¥ ì•„í‚¤í…ì²˜**ë¥¼ ì‚¬ìš©í•œ **í’€ìŠ¤íƒ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜**ì„ êµ¬í˜„í•©ë‹ˆë‹¤:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚â”€â”€â”€â”€â–¶â”‚   Backend    â”‚â”€â”€â”€â”€â–¶â”‚  Database   â”‚
â”‚  (Next.js)  â”‚     â”‚  (FastAPI)   â”‚     â”‚ (PostgreSQL)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”œâ”€â”€â–¶ OCR Service (EasyOCR)
                           â”œâ”€â”€â–¶ LLM Service (Claude API)
                           â”œâ”€â”€â–¶ Image Processing (OpenCV)
                           â””â”€â”€â–¶ Cache (Redis)
```

### ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œ

#### 1. **í”„ë¡ íŠ¸ì—”ë“œ ë ˆì´ì–´** (Next.js + React + TypeScript)

**ëª©ì **: ë¬¸ì„œ ì—…ë¡œë“œ ë° ê²°ê³¼ í™•ì¸ì„ ìœ„í•œ ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤

**ì£¼ìš” ì»´í¬ë„ŒíŠ¸:**
- `ImageUploader`: ë“œë˜ê·¸ ì•¤ ë“œë¡­ìœ¼ë¡œ íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬
- `ResultViewer`: OCR ì¶”ì¶œ ê²°ê³¼ í‘œì‹œ
- `DocumentAnalysis`: ë²ˆì—­ ë° ì¸ì‚¬ì´íŠ¸ í‘œì‹œ
- `HistoryList`: ê³¼ê±° ë¶„ì„ ê¸°ë¡ í‘œì‹œ

**ê¸°ìˆ  ìŠ¤íƒ:**
- **Next.js 14**: ì„œë²„ ì‚¬ì´ë“œ ë Œë”ë§ì„ ì§€ì›í•˜ëŠ” React í”„ë ˆì„ì›Œí¬
- **TypeScript**: íƒ€ì… ì•ˆì „ JavaScript
- **Tailwind CSS**: ìœ í‹¸ë¦¬í‹° ìš°ì„  CSS í”„ë ˆì„ì›Œí¬
- **Axios**: API í˜¸ì¶œì„ ìœ„í•œ HTTP í´ë¼ì´ì–¸íŠ¸

**íŒŒì¼ êµ¬ì¡°:**
```
frontend/src/
â”œâ”€â”€ app/              # Next.js App Router
â”‚   â”œâ”€â”€ page.tsx      # ë©”ì¸ í˜ì´ì§€
â”‚   â””â”€â”€ layout.tsx    # ë£¨íŠ¸ ë ˆì´ì•„ì›ƒ
â”œâ”€â”€ components/       # React ì»´í¬ë„ŒíŠ¸
â”œâ”€â”€ lib/              # ìœ í‹¸ë¦¬í‹°
â”‚   â””â”€â”€ api.ts        # API í´ë¼ì´ì–¸íŠ¸
â”œâ”€â”€ types/            # TypeScript ì •ì˜
â””â”€â”€ hooks/            # ì»¤ìŠ¤í…€ React hooks
```

#### 2. **ë°±ì—”ë“œ ë ˆì´ì–´** (Python + FastAPI)

**ëª©ì **: ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§, OCR ì²˜ë¦¬ ë° AI ë¶„ì„

**ì•„í‚¤í…ì²˜:**
```
app/
â”œâ”€â”€ main.py              # ì• í”Œë¦¬ì¼€ì´ì…˜ ì§„ì…ì 
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ routes.py        # API ì—”ë“œí¬ì¸íŠ¸
â”‚   â””â”€â”€ dependencies.py  # ì˜ì¡´ì„± ì£¼ì…
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ config.py        # ì„¤ì • ê´€ë¦¬
â”‚   â””â”€â”€ security.py      # ì¸ì¦ & JWT
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ ocr_service.py      # í…ìŠ¤íŠ¸ ì¶”ì¶œ
â”‚   â”œâ”€â”€ llm_service.py      # AI ë¶„ì„
â”‚   â”œâ”€â”€ image_service.py    # ì „ì²˜ë¦¬
â”‚   â””â”€â”€ document_service.py # ë¶„ë¥˜
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ database.py      # SQLAlchemy ORM ëª¨ë¸
â”‚   â””â”€â”€ schemas.py       # Pydantic ê²€ì¦
â””â”€â”€ utils/
    â”œâ”€â”€ helpers.py       # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
    â””â”€â”€ validators.py    # ì…ë ¥ ê²€ì¦
```

**ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸:**

| ì—”ë“œí¬ì¸íŠ¸ | ë©”ì„œë“œ | ëª©ì  |
|----------|--------|------|
| `/api/analyze` | POST | ë¬¸ì„œ ì´ë¯¸ì§€ ë¶„ì„ |
| `/api/history` | GET | ì‚¬ìš©ì íˆìŠ¤í† ë¦¬ ì¡°íšŒ |
| `/api/feedback` | POST | ì‚¬ìš©ì í”¼ë“œë°± ì œì¶œ |
| `/health` | GET | í—¬ìŠ¤ ì²´í¬ |

#### 3. **OCR ì„œë¹„ìŠ¤** (EasyOCR)

**ëª©ì **: ë”¥ëŸ¬ë‹ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ

**ì‘ë™ ë°©ì‹:**
1. **ëª¨ë¸ ë¡œë”©**: ë¬¸ì ì¸ì‹ì„ ìœ„í•œ ì‚¬ì „ í›ˆë ¨ëœ ì‹ ê²½ë§
2. **í…ìŠ¤íŠ¸ ê°ì§€**: CRAFT ì•Œê³ ë¦¬ì¦˜ì´ í…ìŠ¤íŠ¸ ì˜ì—­ ì°¾ê¸°
3. **í…ìŠ¤íŠ¸ ì¸ì‹**: CRNN ëª¨ë¸ì´ ë¬¸ì ì¸ì‹
4. **ì–¸ì–´ ê°ì§€**: ë¬¸ì íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ ì–¸ì–´ ì‹ë³„

**ì§€ì› ì–¸ì–´:**
- í•œêµ­ì–´ (ko)
- ì˜ì–´ (en)
- ì¼ë³¸ì–´ (ja)
- ì¤‘êµ­ì–´ ê°„ì²´ (ch_sim)
- ìŠ¤í˜ì¸ì–´ (es)
- í”„ë‘ìŠ¤ì–´ (fr)
- ë…ì¼ì–´ (de)

**ì½”ë“œ ì˜ˆì‹œ:**
```python
class OCRService:
    def __init__(self):
        self.reader = easyocr.Reader(['ko', 'en'], gpu=False)

    async def extract_text(self, image: np.ndarray) -> Dict:
        results = self.reader.readtext(image)

        segments = []
        for bbox, text, confidence in results:
            segments.append({
                "text": text,
                "bbox": bbox,
                "confidence": float(confidence)
            })

        return {
            "full_text": " ".join([s["text"] for s in segments]),
            "segments": segments,
            "detected_languages": self._detect_languages(full_text),
            "confidence": average_confidence
        }
```

#### 4. **ì´ë¯¸ì§€ ì „ì²˜ë¦¬** (OpenCV)

**ëª©ì **: OCR ì •í™•ë„ í–¥ìƒì„ ìœ„í•œ ì´ë¯¸ì§€ í’ˆì§ˆ ê°œì„ 

**ê¸°ë²•:**
1. **ë…¸ì´ì¦ˆ ì œê±°**: Non-local meansë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë…¸ì´ì¦ˆ ì œê±°
2. **ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜**: ìƒ‰ìƒ ë³µì¡ì„± ë‹¨ìˆœí™”
3. **ì ì‘í˜• ì„ê³„ê°’**: í…ìŠ¤íŠ¸ ëŒ€ë¹„ ê°œì„ 
4. **ê¸°ìš¸ê¸° ë³´ì •**: Hough ë³€í™˜ì„ ì‚¬ìš©í•œ íšŒì „ ë³´ì •

**ì½”ë“œ ì˜ˆì‹œ:**
```python
def _enhance_image(self, image: np.ndarray) -> np.ndarray:
    # ë…¸ì´ì¦ˆ ì œê±°
    denoised = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)

    # ê·¸ë ˆì´ìŠ¤ì¼€ì¼
    gray = cv2.cvtColor(denoised, cv2.COLOR_BGR2GRAY)

    # ì ì‘í˜• ì„ê³„ê°’
    thresh = cv2.adaptiveThreshold(
        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY, 11, 2
    )

    # ê¸°ìš¸ê¸° ë³´ì •
    return self._deskew(thresh)
```

#### 5. **LLM ì„œë¹„ìŠ¤** (Claude API)

**ëª©ì **: ë§¥ë½ ì´í•´ ë° ë²ˆì—­ ìƒì„±

**Claude API í†µí•©:**
```python
class LLMService:
    def __init__(self):
        self.client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)

    async def analyze(self, text: str, target_language: str,
                     document_type: str) -> Dict:
        prompt = self._build_prompt(text, target_language, document_type)

        message = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1500,
            messages=[{"role": "user", "content": prompt}]
        )

        return self._parse_response(message.content[0].text)
```

**í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì „ëµ:**
- ë§¥ë½ê³¼ í•¨ê»˜ ì¶”ì¶œëœ OCR í…ìŠ¤íŠ¸ ì œê³µ
- ëª©í‘œ ì–¸ì–´ ë° ë¬¸ì„œ ìœ í˜• ì§€ì •
- êµ¬ì¡°í™”ëœ JSON ì¶œë ¥ ìš”ì²­
- ìš”ì•½, í•µì‹¬ ì •ë³´ ë° ì¡°ì–¸ ìš”ì²­
- ë¬¸ì„œ ìœ í˜•ë³„ íŠ¹ì • ì§€ì¹¨ í¬í•¨

#### 6. **ë°ì´í„°ë² ì´ìŠ¤ ë ˆì´ì–´** (PostgreSQL)

**ìŠ¤í‚¤ë§ˆ ì„¤ê³„:**

```sql
-- ì‚¬ìš©ì í…Œì´ë¸”
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    email VARCHAR UNIQUE NOT NULL,
    hashed_password VARCHAR NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);

-- ë¬¸ì„œ í…Œì´ë¸”
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id),
    document_type VARCHAR,
    source_language VARCHAR,
    target_language VARCHAR,
    original_text TEXT,
    translated_text TEXT,
    key_info JSONB,
    confidence_score FLOAT,
    created_at TIMESTAMP DEFAULT NOW()
);

-- í”¼ë“œë°± í…Œì´ë¸”
CREATE TABLE feedback (
    id SERIAL PRIMARY KEY,
    document_id INTEGER REFERENCES documents(id),
    user_id INTEGER REFERENCES users(id),
    rating INTEGER CHECK (rating BETWEEN 1 AND 5),
    comment TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);
```

### ë°ì´í„° íë¦„

**ì™„ì „í•œ ìš”ì²­ ìƒëª…ì£¼ê¸°:**

```
1. ì‚¬ìš©ìê°€ ì´ë¯¸ì§€ ì—…ë¡œë“œ
   â””â”€â–¶ í”„ë¡ íŠ¸ì—”ë“œê°€ íŒŒì¼ ê²€ì¦ (íƒ€ì…, í¬ê¸°)

2. í”„ë¡ íŠ¸ì—”ë“œê°€ POST ìš”ì²­ ì „ì†¡
   â””â”€â–¶ file, target_language, document_typeì„ í¬í•¨í•œ FormData

3. ë°±ì—”ë“œê°€ ìš”ì²­ ìˆ˜ì‹ 
   â”œâ”€â–¶ ì¸ì¦ ê²€ì¦ (JWT)
   â”œâ”€â–¶ íŒŒì¼ í˜•ì‹ ê²€ì¦
   â””â”€â–¶ ì„ì‹œ ì €ì¥ì†Œì— ì €ì¥

4. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
   â”œâ”€â–¶ ë©”ëª¨ë¦¬ì— ì´ë¯¸ì§€ ë¡œë“œ
   â”œâ”€â–¶ ë…¸ì´ì¦ˆ ì œê±° í•„í„° ì ìš©
   â”œâ”€â–¶ ìµœì  í˜•ì‹ìœ¼ë¡œ ë³€í™˜
   â””â”€â–¶ íšŒì „/ê¸°ìš¸ê¸° ë³´ì •

5. OCR ì²˜ë¦¬
   â”œâ”€â–¶ EasyOCR ëª¨ë¸ ë¡œë“œ
   â”œâ”€â–¶ í…ìŠ¤íŠ¸ ì˜ì—­ ê°ì§€
   â”œâ”€â–¶ ë¬¸ì ì¸ì‹
   â”œâ”€â–¶ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
   â””â”€â–¶ ì–¸ì–´ ê°ì§€

6. ë¬¸ì„œ ë¶„ë¥˜
   â””â”€â–¶ í‚¤ì›Œë“œ ë° íŒ¨í„´ ë¶„ì„

7. LLM ë¶„ì„
   â”œâ”€â–¶ ë§¥ë½ ì¸ì‹ í”„ë¡¬í”„íŠ¸ êµ¬ì¶•
   â”œâ”€â–¶ Claude APIì— ì „ì†¡
   â”œâ”€â–¶ êµ¬ì¡°í™”ëœ ì‘ë‹µ ìˆ˜ì‹ 
   â””â”€â–¶ íŒŒì‹± ë° ê²€ì¦

8. ì‘ë‹µ ì»´íŒŒì¼
   â”œâ”€â–¶ OCR + LLM ê²°ê³¼ ê²°í•©
   â”œâ”€â–¶ ë©”íƒ€ë°ì´í„° ì¶”ê°€ (íƒ€ì´ë°, ì‹ ë¢°ë„)
   â””â”€â–¶ JSONìœ¼ë¡œ í¬ë§·

9. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ (ë¹„ë™ê¸°)
   â””â”€â–¶ ë¬¸ì„œ ë ˆì½”ë“œ ì €ì¥

10. í”„ë¡ íŠ¸ì—”ë“œì— ì‘ë‹µ ì „ì†¡
    â””â”€â–¶ í”„ë¡ íŠ¸ì—”ë“œê°€ ê²°ê³¼ ë Œë”ë§
```

### ì„±ëŠ¥ ìµœì í™”

**ìºì‹± ì „ëµ (Redis):**
```python
# ë™ì¼í•œ ì´ë¯¸ì§€ì— ëŒ€í•œ OCR ê²°ê³¼ ìºì‹±
cache_key = f"ocr:{image_hash}"
if cached := redis.get(cache_key):
    return json.loads(cached)

result = ocr_service.extract_text(image)
redis.setex(cache_key, 3600, json.dumps(result))  # 1ì‹œê°„ TTL
```

**ë¹„ë™ê¸° ì²˜ë¦¬:**
```python
# ë¹„ì°¨ë‹¨ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
async def save_document(doc_data: dict):
    async with db.session() as session:
        document = Document(**doc_data)
        session.add(document)
        await session.commit()

# Fire and forget
asyncio.create_task(save_document(data))
```

### í…ŒìŠ¤íŠ¸ ì „ëµ

**ë°±ì—”ë“œ í…ŒìŠ¤íŠ¸ (pytest):**
```python
def test_ocr_extraction(test_image):
    service = OCRService()
    result = await service.extract_text(test_image)

    assert result["full_text"]
    assert len(result["segments"]) > 0
    assert result["confidence"] > 0.8

def test_api_analyze_endpoint(client, test_image_file):
    response = client.post(
        "/api/analyze",
        files={"file": test_image_file},
        data={"target_language": "en"}
    )

    assert response.status_code == 200
    assert "ocr_result" in response.json()
    assert "analysis" in response.json()
```

**í”„ë¡ íŠ¸ì—”ë“œ í…ŒìŠ¤íŠ¸ (Jest + React Testing Library):**
```typescript
test('ImageUploaderê°€ íŒŒì¼ ì—…ë¡œë“œë¥¼ ìˆ˜ë½í•¨', async () => {
  render(<ImageUploader onAnalysisComplete={jest.fn()} />)

  const file = new File(['image'], 'test.jpg', { type: 'image/jpeg' })
  const input = screen.getByLabelText(/upload/i)

  await userEvent.upload(input, file)

  expect(screen.getByText('test.jpg')).toBeInTheDocument()
})
```

### ë°°í¬

**Docker Compose ì„¤ì •:**
```yaml
services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
    depends_on:
      - postgres
      - redis

  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    depends_on:
      - backend
```

**í”„ë¡œë•ì…˜ ê³ ë ¤ì‚¬í•­:**
- ê´€ë¦¬í˜• ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš© (AWS RDS, Google Cloud SQL)
- ì •ì  ìì‚°ìš© CDN êµ¬í˜„ (CloudFront, Cloudflare)
- ë¡œë“œ ë°¸ëŸ°ì‹± ì¶”ê°€ (nginx, AWS ALB)
- SSL/TLS ì¸ì¦ì„œ í™œì„±í™”
- ëª¨ë‹ˆí„°ë§ ì„¤ì • (Prometheus, Grafana)
- ë¡œê¹… êµ¬í˜„ (ELK ìŠ¤íƒ)

---

## ê°œë°œììš©

### ê°œë°œ ì›Œí¬í”Œë¡œìš°

#### ë¡œì»¬ ê°œë°œ ì„¤ì •

**1. í´ë¡  ë° í™˜ê²½ ì„¤ì •:**
```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/dddoing/ocrproject.git
cd ocrproject

# í…œí”Œë¦¿ì—ì„œ .env ìƒì„±
cp .env.example .env

# API í‚¤ ì¶”ê°€
echo "ANTHROPIC_API_KEY=sk-ant-your-key" >> .env
```

**2. ë°±ì—”ë“œ ê°œë°œ:**
```bash
cd backend

# ê°€ìƒ í™˜ê²½ ìƒì„±
python3.10 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ê°œë°œ ì˜ì¡´ì„± ì„¤ì¹˜
pip install pytest pytest-cov black flake8 mypy

# ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰ (êµ¬í˜„ ì‹œ)
alembic upgrade head

# í•« ë¦¬ë¡œë“œë¡œ ê°œë°œ ì„œë²„ ì‹œì‘
uvicorn app.main:app --reload --port 8000 --log-level debug
```

**3. í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œ:**
```bash
cd frontend

# ì˜ì¡´ì„± ì„¤ì¹˜
npm install

# ê°œë°œ ì˜ì¡´ì„± ì„¤ì¹˜ (package.jsonì— ì—†ëŠ” ê²½ìš°)
npm install --save-dev @types/jest jest

# ê°œë°œ ì„œë²„ ì‹œì‘
npm run dev

# ë³„ë„ í„°ë¯¸ë„ì—ì„œ: íƒ€ì… ì—ëŸ¬ ê°ì‹œ
npm run type-check -- --watch
```

**4. ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •:**
```bash
# PostgreSQL ì‹œì‘
docker run --name dev-postgres \
  -e POSTGRES_USER=devuser \
  -e POSTGRES_PASSWORD=devpass \
  -e POSTGRES_DB=devdb \
  -p 5432:5432 \
  -d postgres:15

# Redis ì‹œì‘
docker run --name dev-redis \
  -p 6379:6379 \
  -d redis:7-alpine

# ì—°ê²° í™•ì¸
psql postgresql://devuser:devpass@localhost:5432/devdb -c "SELECT version();"
redis-cli ping
```

### ì½”ë“œ ìŠ¤íƒ€ì¼ ë° í‘œì¤€

**Python (ë°±ì—”ë“œ):**
```bash
# ì½”ë“œ í¬ë§·
black app/ tests/

# ë¦°íŠ¸
flake8 app/ tests/ --max-line-length=100

# íƒ€ì… ì²´í‚¹
mypy app/

# ëª¨ë“  ì²´í¬ ì‹¤í–‰
black app/ && flake8 app/ && mypy app/ && pytest
```

**ì„¤ì • íŒŒì¼:**

`.flake8`:
```ini
[flake8]
max-line-length = 100
exclude = venv,.git,__pycache__
ignore = E203,W503
```

`pyproject.toml` (Blackìš©):
```toml
[tool.black]
line-length = 100
target-version = ['py310']
```

**TypeScript/React (í”„ë¡ íŠ¸ì—”ë“œ):**
```bash
# ë¦°íŠ¸
npm run lint

# í¬ë§· (Prettier ì‚¬ìš© ì‹œ)
npm run format

# íƒ€ì… ì²´í¬
npm run type-check
```

### ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€

#### ì˜ˆì‹œ: ìƒˆë¡œìš´ ë¬¸ì„œ ìœ í˜• "Invoice" ì¶”ê°€

**1. Document Service ì—…ë°ì´íŠ¸ (ë°±ì—”ë“œ):**

`backend/app/services/document_service.py`:
```python
async def classify(self, text: str, segments: List[Dict]) -> str:
    text_lower = text.lower()

    # ì²­êµ¬ì„œ í‚¤ì›Œë“œ ì¶”ê°€
    invoice_keywords = [
        'invoice', 'bill to', 'invoice number', 'due date',
        'amount due', 'subtotal', 'net amount'
    ]
    if any(keyword in text_lower for keyword in invoice_keywords):
        return "invoice"

    # ... ê¸°ì¡´ ì½”ë“œ ...
```

**2. Invoice íŠ¹ì • í”„ë¡¬í”„íŠ¸ë¡œ LLM Service ì—…ë°ì´íŠ¸:**

`backend/app/services/llm_service.py`:
```python
def _get_type_specific_instructions(self, document_type: str) -> str:
    instructions = {
        # ... ê¸°ì¡´ íƒ€ì… ...
        "invoice": """
        - ì²­êµ¬ì„œ ë²ˆí˜¸, ë‚ ì§œ, ë§Œê¸°ì¼ ì¶”ì¶œ
        - ìˆ˜ëŸ‰ê³¼ ê°€ê²©ì„ í¬í•¨í•œ ëª¨ë“  í•­ëª© ë‚˜ì—´
        - ì†Œê³„, ì„¸ê¸ˆ, ì´ì•¡ ê³„ì‚° ê²€ì¦
        - ì§€ë¶ˆ ì¡°ê±´ ë° ë°©ë²• ì‹ë³„
        - ì—°ì²´ ë˜ëŠ” ë¹„ì •ìƒì ì¸ ìš”ê¸ˆ í‘œì‹œ
        """,
    }
    return instructions.get(document_type, "")
```

**3. TypeScript íƒ€ì… ì¶”ê°€ (í”„ë¡ íŠ¸ì—”ë“œ):**

`frontend/src/types/index.ts`:
```typescript
export type DocumentType =
  | 'menu'
  | 'contract'
  | 'receipt'
  | 'invoice'  // ìƒˆ íƒ€ì…
  | 'medical'
  | 'sign'
  | 'general';

export interface InvoiceInfo extends KeyInfo {
  invoice_number?: string;
  due_date?: string;
  payment_terms?: string;
  line_items?: Array<{
    description: string;
    quantity: number;
    unit_price: string;
    total: string;
  }>;
}
```

**4. Invoice íŠ¹ì • ì»´í¬ë„ŒíŠ¸ ìƒì„±:**

`frontend/src/components/InvoiceAnalysis.tsx`:
```typescript
export default function InvoiceAnalysis({ analysis }: { analysis: Analysis }) {
  const invoiceInfo = analysis.key_info as InvoiceInfo;

  return (
    <div className="bg-white rounded-lg p-6">
      <h3 className="text-xl font-bold mb-4">ì²­êµ¬ì„œ ìƒì„¸ ì •ë³´</h3>

      <div className="grid grid-cols-2 gap-4">
        <InfoItem label="ì²­êµ¬ì„œ ë²ˆí˜¸" value={invoiceInfo.invoice_number} />
        <InfoItem label="ë§Œê¸°ì¼" value={invoiceInfo.due_date} />
        <InfoItem label="ì§€ë¶ˆ ì¡°ê±´" value={invoiceInfo.payment_terms} />
      </div>

      <LineItemsTable items={invoiceInfo.line_items} />

      {analysis.advice && (
        <Alert type="warning">{analysis.advice}</Alert>
      )}
    </div>
  );
}
```

**5. DocumentAnalysis ì»´í¬ë„ŒíŠ¸ ì—…ë°ì´íŠ¸:**

`frontend/src/components/DocumentAnalysis.tsx`:
```typescript
import InvoiceAnalysis from './InvoiceAnalysis';

export default function DocumentAnalysis({ analysis }: DocumentAnalysisProps) {
  // ë¬¸ì„œ ìœ í˜•ë³„ ì»´í¬ë„ŒíŠ¸ ë Œë”ë§
  if (analysis.document_type === 'invoice') {
    return <InvoiceAnalysis analysis={analysis} />;
  }

  // ... ê¸°ì¡´ ì½”ë“œ ...
}
```

**6. í…ŒìŠ¤íŠ¸ ì¶”ê°€:**

`backend/tests/test_invoice.py`:
```python
def test_invoice_classification():
    service = DocumentService()
    text = "INVOICE #12345\nBill To: John Doe\nAmount Due: $500"

    result = await service.classify(text, [])

    assert result == "invoice"

def test_invoice_analysis():
    service = LLMService()
    text = "ì²­êµ¬ì„œ ìƒì„¸ ì •ë³´..."

    result = await service.analyze(text, "en", "invoice", [])

    assert "invoice_number" in result["key_info"]
    assert "due_date" in result["key_info"]
```

`frontend/src/components/__tests__/InvoiceAnalysis.test.tsx`:
```typescript
test('ì²­êµ¬ì„œ ìƒì„¸ ì •ë³´ ë Œë”ë§', () => {
  const mockAnalysis = {
    document_type: 'invoice',
    key_info: {
      invoice_number: 'INV-12345',
      due_date: '2025-12-31'
    }
  };

  render(<InvoiceAnalysis analysis={mockAnalysis} />);

  expect(screen.getByText('INV-12345')).toBeInTheDocument();
  expect(screen.getByText('2025-12-31')).toBeInTheDocument();
});
```

### ë””ë²„ê¹… ê°€ì´ë“œ

**ë°±ì—”ë“œ ë””ë²„ê¹…:**

1. **ë””ë²„ê·¸ ë¡œê¹… í™œì„±í™”:**
```python
# app/main.py
import logging

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)
logger.debug("ë””ë²„ê·¸ ë©”ì‹œì§€")
```

2. **FastAPI ë””ë²„ê·¸ ëª¨ë“œ ì‚¬ìš©:**
```bash
uvicorn app.main:app --reload --log-level debug
```

3. **ëŒ€í™”í˜• ë””ë²„ê¹… (pdb):**
```python
# ì¤‘ë‹¨ì  ì¶”ê°€
import pdb; pdb.set_trace()

# ë˜ëŠ” Python 3.7+ì—ì„œ breakpoint() ì‚¬ìš©
breakpoint()
```

4. **ê°œë³„ ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸:**
```python
# OCRì„ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
from app.services.ocr_service import OCRService
import cv2

service = OCRService()
image = cv2.imread("test_image.jpg")
result = await service.extract_text(image)
print(result)
```

**í”„ë¡ íŠ¸ì—”ë“œ ë””ë²„ê¹…:**

1. **React Developer Tools:**
   - ë¸Œë¼ìš°ì € í™•ì¥ í”„ë¡œê·¸ë¨ ì„¤ì¹˜
   - ì»´í¬ë„ŒíŠ¸ ê³„ì¸µ êµ¬ì¡° ê²€ì‚¬
   - props ë° state í™•ì¸

2. **ì½˜ì†” ë””ë²„ê¹…:**
```typescript
// ë””ë²„ê·¸ ë¡œê·¸ ì¶”ê°€
console.log('ë¶„ì„ ê²°ê³¼:', analysisResult);

// debugger ë¬¸ ì‚¬ìš©
debugger;

// API í˜¸ì¶œ ë¡œê¹…
axios.interceptors.request.use(request => {
  console.log('ìš”ì²­ ì‹œì‘', request)
  return request
})
```

3. **ë„¤íŠ¸ì›Œí¬ íƒ­:**
   - API ìš”ì²­/ì‘ë‹µ ëª¨ë‹ˆí„°ë§
   - í˜ì´ë¡œë“œ ë° í—¤ë” í™•ì¸
   - ì‘ë‹µ ìƒíƒœ ì½”ë“œ ê²€ì¦

**Docker ë””ë²„ê¹…:**

```bash
# ë¡œê·¸ ë³´ê¸°
docker-compose logs -f backend
docker-compose logs -f frontend

# ì»¨í…Œì´ë„ˆ ì…¸ ì ‘ê·¼
docker-compose exec backend bash
docker-compose exec frontend sh

# ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
docker-compose ps

# íŠ¹ì • ì„œë¹„ìŠ¤ ì¬ì‹œì‘
docker-compose restart backend
```

### ë³´ì•ˆ ëª¨ë²” ì‚¬ë¡€

**1. API í‚¤ ê´€ë¦¬:**
```python
# API í‚¤ë¥¼ í•˜ë“œì½”ë”©í•˜ì§€ ë§ˆì„¸ìš”
# âŒ ë‚˜ì¨
ANTHROPIC_API_KEY = "sk-ant-api03-..."

# âœ… ì¢‹ìŒ
from app.core.config import settings
api_key = settings.ANTHROPIC_API_KEY
```

**2. ì…ë ¥ ê²€ì¦:**
```python
# íŒŒì¼ ì—…ë¡œë“œ ê²€ì¦
from app.utils.validators import validate_file_extension, validate_file_size

if not validate_file_extension(file.filename):
    raise HTTPException(400, "ìœ íš¨í•˜ì§€ ì•Šì€ íŒŒì¼ íƒ€ì…")

if not validate_file_size(len(await file.read())):
    raise HTTPException(400, "íŒŒì¼ì´ ë„ˆë¬´ í¼")
```

**3. SQL ì¸ì ì…˜ ë°©ì§€:**
```python
# ORM (SQLAlchemy) ì‚¬ìš© - ìë™ìœ¼ë¡œ SQL ì¸ì ì…˜ ë°©ì§€
# âœ… ì¢‹ìŒ
users = session.query(User).filter(User.email == email).all()

# âŒ ë‚˜ì¨ - ì ˆëŒ€ ì´ë ‡ê²Œ í•˜ì§€ ë§ˆì„¸ìš”
query = f"SELECT * FROM users WHERE email = '{email}'"
```

**4. XSS ë°©ì§€:**
```typescript
// ReactëŠ” ìë™ìœ¼ë¡œ ì½˜í…ì¸ ë¥¼ ì´ìŠ¤ì¼€ì´í”„
// âœ… ì•ˆì „
<p>{userInput}</p>

// âŒ ìœ„í—˜ - ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì½˜í…ì¸ ì—ë§Œ ì‚¬ìš©
<div dangerouslySetInnerHTML={{__html: userInput}} />
```

**5. ì†ë„ ì œí•œ:**
```python
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@app.post("/api/analyze")
@limiter.limit("10/minute")
async def analyze(request: Request):
    # ...
```

### ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

**ì• í”Œë¦¬ì¼€ì´ì…˜ ëª¨ë‹ˆí„°ë§:**

```python
# app/main.py
from prometheus_client import Counter, Histogram
import time

# ë©”íŠ¸ë¦­
request_count = Counter('app_requests_total', 'ì´ ìš”ì²­ ìˆ˜')
request_duration = Histogram('app_request_duration_seconds', 'ìš”ì²­ ì§€ì† ì‹œê°„')

@app.middleware("http")
async def monitor_requests(request: Request, call_next):
    request_count.inc()

    start_time = time.time()
    response = await call_next(request)
    duration = time.time() - start_time

    request_duration.observe(duration)

    return response
```

**êµ¬ì¡°í™”ëœ ë¡œê¹…:**

```python
import structlog

logger = structlog.get_logger()

logger.info(
    "document_analyzed",
    document_type=doc_type,
    confidence=confidence,
    processing_time=duration,
    user_id=user_id
)
```

### CI/CD íŒŒì´í”„ë¼ì¸

**GitHub Actions ì˜ˆì‹œ:**

`.github/workflows/ci.yml`:
```yaml
name: CI/CD Pipeline

on: [push, pull_request]

jobs:
  backend-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Python ì„¤ì •
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'

      - name: ì˜ì¡´ì„± ì„¤ì¹˜
        run: |
          cd backend
          pip install -r requirements.txt
          pip install pytest pytest-cov

      - name: í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        run: |
          cd backend
          pytest --cov=app tests/

      - name: ë¦°íŠ¸
        run: |
          cd backend
          flake8 app/

  frontend-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Node.js ì„¤ì •
        uses: actions/setup-node@v2
        with:
          node-version: '18'

      - name: ì˜ì¡´ì„± ì„¤ì¹˜
        run: |
          cd frontend
          npm ci

      - name: íƒ€ì… ì²´í¬
        run: |
          cd frontend
          npm run type-check

      - name: ë¦°íŠ¸
        run: |
          cd frontend
          npm run lint

      - name: í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        run: |
          cd frontend
          npm test

  deploy:
    needs: [backend-tests, frontend-tests]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - name: í”„ë¡œë•ì…˜ ë°°í¬
        run: |
          # ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
          echo "í”„ë¡œë•ì…˜ ë°°í¬ ì¤‘..."
```

---

## ì „ë¬¸ê°€ ë° ê³ ê¸‰ ì‚¬ìš©ììš©

### ì‹¬ì¸µ ê¸°ìˆ  ë¶„ì„

#### OCR ì•„í‚¤í…ì²˜ ì‹¬ì¸µ ë¶„ì„

**EasyOCR íŒŒì´í”„ë¼ì¸:**

1. **í…ìŠ¤íŠ¸ ê°ì§€ (CRAFT - Character Region Awareness For Text detection)**
   ```python
   # CRAFTëŠ” ë‹¤ìŒì„ ì˜ˆì¸¡í•˜ëŠ” ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§ ì‚¬ìš©:
   # - ë¬¸ì ì˜ì—­ ì ìˆ˜ (íˆíŠ¸ë§µ)
   # - ì¹œí™”ë„ ì ìˆ˜ (ë¬¸ì ì—°ê²°)

   # ì•„í‚¤í…ì²˜:
   VGG-16 Backbone â†’ Feature Pyramid Network â†’
   Skip Connections â†’ Final Conv Layer â†’
   Region Score Map + Affinity Score Map

   # í›„ì²˜ë¦¬:
   # 1. ë¬¸ì ê·¸ë£¹í™”ë¥¼ ìœ„í•œ Watershed ì•Œê³ ë¦¬ì¦˜
   # 2. ì—°ê²° ìš”ì†Œ ë¶„ì„
   # 3. ë°”ìš´ë”© ë°•ìŠ¤ ìƒì„±
   ```

2. **í…ìŠ¤íŠ¸ ì¸ì‹ (CRNN - Convolutional Recurrent Neural Network)**
   ```python
   # ì•„í‚¤í…ì²˜:
   Convolutional Layers (íŠ¹ì§• ì¶”ì¶œ) â†’
   Recurrent Layers (ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì„ ìœ„í•œ LSTM/GRU) â†’
   Transcription Layer (CTC - Connectionist Temporal Classification)

   # CTCëŠ” ì •ë ¬ì´ í•„ìš” ì—†ëŠ” í›ˆë ¨ ê°€ëŠ¥:
   # - ê°€ë³€ ê¸¸ì´ ì‹œí€€ìŠ¤ ì²˜ë¦¬
   # - ë¬¸ì ìˆ˜ì¤€ ì£¼ì„ ë¶ˆí•„ìš”
   # - ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ ë¬¸ì ì‹œí€€ìŠ¤ ì¶œë ¥
   ```

**ì»¤ìŠ¤í…€ OCR ìµœì í™”:**

```python
class OptimizedOCRService:
    def __init__(self):
        # GPU ê°€ì† ì‚¬ìš©
        self.reader = easyocr.Reader(
            ['ko', 'en'],
            gpu=True,
            model_storage_directory='/models',
            download_enabled=False  # ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ëª¨ë¸
        )

        # ì»¤ìŠ¤í…€ ì¸ì‹ ë„¤íŠ¸ì›Œí¬
        self.reader.setModelLanguage(
            'ko',
            '/models/custom_korean_model.pth'
        )

    async def extract_text_batch(self, images: List[np.ndarray]) -> List[Dict]:
        """íš¨ìœ¨ì„±ì„ ìœ„í•œ ë°°ì¹˜ ì²˜ë¦¬"""
        # ë‹¤ì¤‘ ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
        with multiprocessing.Pool(processes=4) as pool:
            results = pool.map(self._process_single, images)
        return results

    def _apply_advanced_preprocessing(self, image: np.ndarray) -> np.ndarray:
        """ê³ ê¸‰ ì „ì²˜ë¦¬ ê¸°ë²•"""
        # 1. ìƒ‰ìƒ ê³µê°„ ì •ê·œí™”
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        l = clahe.apply(l)
        enhanced = cv2.merge([l,a,b])
        enhanced = cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)

        # 2. í˜•íƒœí•™ì  ì—°ì‚°
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))
        morph = cv2.morphologyEx(enhanced, cv2.MORPH_CLOSE, kernel)

        # 3. ê·¸ë¦¼ì ì œê±°
        dilated = cv2.dilate(morph, kernel, iterations=1)
        median = cv2.medianBlur(dilated, 21)
        diff = 255 - cv2.absdiff(morph, median)

        # 4. Otsuë¥¼ ì‚¬ìš©í•œ ì´ì§„í™”
        gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)
        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

        return binary
```

#### LLM í†µí•© ìµœì í™”

**ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§:**

```python
class AdvancedLLMService:
    def __init__(self):
        self.client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)

        # Few-shot ì˜ˆì‹œê°€ ìˆëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.templates = {
            "menu": self._load_template("menu_template.json"),
            "contract": self._load_template("contract_template.json"),
            # ...
        }

    async def analyze_with_chain_of_thought(
        self,
        text: str,
        target_language: str,
        document_type: str
    ) -> Dict:
        """ë³µì¡í•œ ë¶„ì„ì„ ìœ„í•œ ì‚¬ê³ ì˜ ì—°ì‡„ í”„ë¡¬í”„íŒ… ì‚¬ìš©"""

        # 1ë‹¨ê³„: ë¬¸ì„œ ì´í•´
        understanding_prompt = f"""
        ë¨¼ì €, ì´ {document_type} ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì„¸ìš”:
        {text}

        ë‹¨ê³„ë³„ë¡œ ìƒê°í•˜ì„¸ìš”:
        1. ì´ ë¬¸ì„œì˜ ì£¼ìš” ëª©ì ì€ ë¬´ì—‡ì¸ê°€?
        2. ëŒ€ìƒ ë…ìëŠ” ëˆ„êµ¬ì¸ê°€?
        3. ì£¼ìš” ì„¹ì…˜ì€ ë¬´ì—‡ì¸ê°€?
        4. ì• ë§¤í•˜ê±°ë‚˜ ë¶ˆëª…í™•í•œ ë¶€ë¶„ì´ ìˆëŠ”ê°€?

        ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”:
        """

        understanding = await self._call_claude(understanding_prompt)

        # 2ë‹¨ê³„: ë§¥ë½ì„ ê³ ë ¤í•œ ë²ˆì—­
        translation_prompt = f"""
        ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ:
        {understanding}

        ì´ì œ ë¬¸ì„œë¥¼ {target_language}ë¡œ ë²ˆì—­í•˜ì„¸ìš”. ê³ ë ¤ì‚¬í•­:
        - ë¬¸í™”ì  ë§¥ë½ê³¼ ê´€ìš©êµ¬
        - ì „ë¬¸ ìš©ì–´
        - ì–´ì¡° ë° ê²©ì‹ ìˆ˜ì¤€
        - ëŒ€ìƒ ë…ì ê¸°ëŒ€

        ë²ˆì—­:
        """

        translation = await self._call_claude(translation_prompt)

        # 3ë‹¨ê³„: í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        extraction_prompt = f"""
        ë²ˆì—­ëœ ë¬¸ì„œì—ì„œ:
        {translation}

        JSON í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ:
        {{
            "document_metadata": {{...}},
            "key_entities": {{...}},
            "important_dates": [...],
            "financial_info": {{...}},
            "action_items": [...]
        }}
        """

        key_info = await self._call_claude(extraction_prompt)

        # 4ë‹¨ê³„: ë¬¸ë§¥ì  ì¡°ì–¸
        advice_prompt = f"""
        ì´ {document_type}ë¥¼ ê³ ë ¤í•˜ì—¬:
        - ë²ˆì—­: {translation}
        - í•µì‹¬ ì •ë³´: {key_info}

        ì´ ë¬¸ì„œ ìœ í˜•ì— íŠ¹ì •í•œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”.
        ê³ ë ¤ì‚¬í•­: ì¼ë°˜ì ì¸ í•¨ì •, ë¬¸í™”ì  ê³ ë ¤ì‚¬í•­, ë²•ì  ì˜í–¥.
        """

        advice = await self._call_claude(advice_prompt)

        return {
            "understanding": understanding,
            "translation": translation,
            "key_info": json.loads(key_info),
            "advice": advice
        }

    async def _call_claude(self, prompt: str, cache: bool = True) -> str:
        """ì¼ë°˜ì ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìœ„í•œ ìºì‹±ê³¼ í•¨ê»˜ Claude í˜¸ì¶œ"""

        if cache:
            cache_key = hashlib.md5(prompt.encode()).hexdigest()
            if cached := await self.redis.get(cache_key):
                return cached

        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=2000,
            temperature=0.7,  # ì‘ì—…ì— ë”°ë¼ ì¡°ì •
            system="ë‹¹ì‹ ì€ ì „ë¬¸ ë‹¤êµ­ì–´ ë¬¸ì„œ ë¶„ì„ê°€ì…ë‹ˆë‹¤.",
            messages=[{"role": "user", "content": prompt}]
        )

        result = response.content[0].text

        if cache:
            await self.redis.setex(cache_key, 3600, result)

        return result
```

**ì‹¤ì‹œê°„ UXë¥¼ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ:**

```python
@app.post("/api/analyze-stream")
async def analyze_stream(file: UploadFile):
    async def generate():
        # OCR ë‹¨ê³„
        yield json.dumps({"phase": "ocr", "status": "processing"}) + "\n"
        ocr_result = await ocr_service.extract_text(image)
        yield json.dumps({"phase": "ocr", "status": "complete", "data": ocr_result}) + "\n"

        # ë²ˆì—­ ë‹¨ê³„
        yield json.dumps({"phase": "translation", "status": "processing"}) + "\n"

        # í† í°ë³„ ë²ˆì—­ ìŠ¤íŠ¸ë¦¬ë°
        async with client.messages.stream(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1500,
            messages=[{"role": "user", "content": prompt}]
        ) as stream:
            async for text in stream.text_stream:
                yield json.dumps({"phase": "translation", "token": text}) + "\n"

        # ìµœì¢… ë¶„ì„
        yield json.dumps({"phase": "complete", "data": final_result}) + "\n"

    return StreamingResponse(generate(), media_type="application/x-ndjson")
```

#### ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

**ê³ ê¸‰ ì¿¼ë¦¬ ìµœì í™”:**

```python
# SQLAlchemy ì¿¼ë¦¬ ìµœì í™” ì‚¬ìš©
from sqlalchemy.orm import selectinload, joinedload

# N+1 ì¿¼ë¦¬ ë°©ì§€ë¥¼ ìœ„í•œ Eager loading
documents = session.query(Document).options(
    selectinload(Document.user),
    selectinload(Document.feedback)
).filter(
    Document.created_at >= datetime.now() - timedelta(days=30)
).all()

# ì„±ëŠ¥ì„ ìœ„í•œ ì¸ë±ìŠ¤ ì¿¼ë¦¬
# ìì£¼ ì¿¼ë¦¬ë˜ëŠ” ì—´ì— ì¸ë±ìŠ¤ ì¶”ê°€
class Document(Base):
    __tablename__ = "documents"

    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, index=True)  # ì¸ë±ìŠ¤ë¨
    document_type = Column(String, index=True)  # ì¸ë±ìŠ¤ë¨
    created_at = Column(DateTime, index=True)  # ì‹œê°„ ê¸°ë°˜ ì¿¼ë¦¬ìš© ì¸ë±ìŠ¤

    # ì¼ë°˜ì ì¸ ì¿¼ë¦¬ íŒ¨í„´ì„ ìœ„í•œ ë³µí•© ì¸ë±ìŠ¤
    __table_args__ = (
        Index('idx_user_date', 'user_id', 'created_at'),
    )

# ë°ì´í„°ë² ì´ìŠ¤ ìˆ˜ì¤€ ì „ì²´ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‚¬ìš©
from sqlalchemy.dialects.postgresql import TSVECTOR

class Document(Base):
    # ... ê¸°ì¡´ ì—´ ...

    search_vector = Column(TSVECTOR)  # PostgreSQL ì „ì²´ í…ìŠ¤íŠ¸ ê²€ìƒ‰

    __table_args__ = (
        Index(
            'idx_search_vector',
            'search_vector',
            postgresql_using='gin'
        ),
    )

# ë¬¸ì„œ ê²€ìƒ‰
results = session.query(Document).filter(
    Document.search_vector.match('contract legal terms')
).all()
```

**ì—°ê²° í’€ë§:**

```python
# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìµœì í™”
from sqlalchemy.pool import QueuePool

engine = create_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=20,  # ìµœëŒ€ ì—°ê²°
    max_overflow=40,  # í—ˆìš©ë˜ëŠ” ì¶”ê°€ ì—°ê²°
    pool_pre_ping=True,  # ì‚¬ìš© ì „ ì—°ê²° í™•ì¸
    pool_recycle=3600,  # ë§¤ì‹œê°„ ì—°ê²° ì¬í™œìš©
    echo=False  # í”„ë¡œë•ì…˜ì—ì„œ SQL ë¡œê¹… ë¹„í™œì„±í™”
)
```

#### í™•ì¥ì„± ì•„í‚¤í…ì²˜

**ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ë¶„í•´:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Gateway   â”‚ (Kong, AWS API Gateway)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          â”‚          â”‚           â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Auth  â”‚ â”‚  OCR  â”‚ â”‚   LLM   â”‚ â”‚ Document  â”‚
â”‚Service â”‚ â”‚Serviceâ”‚ â”‚ Service â”‚ â”‚  Service  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚          â”‚          â”‚            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
              â”‚  Message  â”‚ (RabbitMQ, Kafka)
              â”‚   Queue   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ë¹„ë™ê¸° ì‘ì—… í (Celery):**

```python
# tasks.py
from celery import Celery

celery_app = Celery(
    'ocr_tasks',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/1'
)

@celery_app.task(bind=True, max_retries=3)
def process_document_async(self, image_path: str, user_id: int):
    try:
        # ì¥ì‹œê°„ ì‹¤í–‰ OCR í”„ë¡œì„¸ìŠ¤
        image = cv2.imread(image_path)
        ocr_result = ocr_service.extract_text(image)

        # LLM ë¶„ì„
        analysis = llm_service.analyze(ocr_result['full_text'])

        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        save_document(user_id, ocr_result, analysis)

        # WebSocketì„ í†µí•´ ì‚¬ìš©ìì—ê²Œ ì•Œë¦¼
        notify_user(user_id, {"status": "complete", "result": analysis})

    except Exception as exc:
        # ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„
        raise self.retry(exc=exc, countdown=2 ** self.request.retries)

# API ì—”ë“œí¬ì¸íŠ¸
@app.post("/api/analyze-async")
async def analyze_async(file: UploadFile, user_id: int):
    # íŒŒì¼ ì €ì¥
    file_path = await save_upload(file)

    # ì‘ì—… ëŒ€ê¸°ì—´
    task = process_document_async.delay(file_path, user_id)

    return {"task_id": task.id, "status": "queued"}
```

**Kubernetesë¥¼ ì‚¬ìš©í•œ ë¡œë“œ ë°¸ëŸ°ì‹±:**

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ocr-backend
spec:
  replicas: 5
  selector:
    matchLabels:
      app: ocr-backend
  template:
    metadata:
      labels:
        app: ocr-backend
    spec:
      containers:
      - name: backend
        image: ocrproject/backend:latest
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: url
---
apiVersion: v1
kind: Service
metadata:
  name: ocr-backend-service
spec:
  type: LoadBalancer
  selector:
    app: ocr-backend
  ports:
  - port: 80
    targetPort: 8000
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ocr-backend-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ocr-backend
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

#### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹

**Locustë¥¼ ì‚¬ìš©í•œ ë¶€í•˜ í…ŒìŠ¤íŠ¸:**

```python
# locustfile.py
from locust import HttpUser, task, between
import random

class DocumentAnalysisUser(HttpUser):
    wait_time = between(1, 3)

    @task(3)
    def analyze_document(self):
        files = {'file': open('test_images/sample.jpg', 'rb')}
        data = {'target_language': 'en'}

        with self.client.post(
            "/api/analyze",
            files=files,
            data=data,
            catch_response=True
        ) as response:
            if response.status_code == 200:
                response.success()
            else:
                response.failure(f"ìƒíƒœ {response.status_code}ë¡œ ì‹¤íŒ¨")

    @task(1)
    def get_history(self):
        self.client.get("/api/history")

# ì‹¤í–‰: locust -f locustfile.py --host=http://localhost:8000
```

**ì„±ëŠ¥ ë©”íŠ¸ë¦­:**

```python
from prometheus_client import Counter, Histogram, Gauge

# ë©”íŠ¸ë¦­ ì •ì˜
ocr_processing_time = Histogram(
    'ocr_processing_seconds',
    'OCR ì²˜ë¦¬ì— ì†Œìš”ëœ ì‹œê°„',
    buckets=[0.5, 1.0, 2.0, 3.0, 5.0, 10.0]
)

llm_api_calls = Counter(
    'llm_api_calls_total',
    'ì´ LLM API í˜¸ì¶œ',
    ['model', 'status']
)

active_analyses = Gauge(
    'active_analyses',
    'í˜„ì¬ ë¶„ì„ ì¤‘ì¸ ë¬¸ì„œ ìˆ˜'
)

# ì½”ë“œ ê³„ì¸¡
@ocr_processing_time.time()
async def extract_text(image):
    active_analyses.inc()
    try:
        result = await ocr_service.extract_text(image)
        return result
    finally:
        active_analyses.dec()

# LLM í˜¸ì¶œ ì¶”ì 
try:
    response = await llm_service.analyze(text)
    llm_api_calls.labels(model='claude-3.5', status='success').inc()
except Exception:
    llm_api_calls.labels(model='claude-3.5', status='error').inc()
    raise
```

#### ê³ ê¸‰ ë³´ì•ˆ

**OAuth2 êµ¬í˜„:**

```python
from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm
from jose import JWTError, jwt
from datetime import datetime, timedelta

oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

def create_access_token(data: dict, expires_delta: timedelta = None):
    to_encode = data.copy()
    expire = datetime.utcnow() + (expires_delta or timedelta(minutes=15))
    to_encode.update({"exp": expire, "sub": data["user_id"]})

    encoded_jwt = jwt.encode(
        to_encode,
        settings.SECRET_KEY,
        algorithm=settings.ALGORITHM
    )
    return encoded_jwt

@app.post("/token")
async def login(form_data: OAuth2PasswordRequestForm = Depends()):
    user = authenticate_user(form_data.username, form_data.password)
    if not user:
        raise HTTPException(status_code=401, detail="ìœ íš¨í•˜ì§€ ì•Šì€ ìê²© ì¦ëª…")

    access_token = create_access_token(
        data={"user_id": user.id, "scopes": form_data.scopes}
    )

    return {"access_token": access_token, "token_type": "bearer"}

async def get_current_user(token: str = Depends(oauth2_scheme)):
    try:
        payload = jwt.decode(
            token,
            settings.SECRET_KEY,
            algorithms=[settings.ALGORITHM]
        )
        user_id: str = payload.get("sub")
        if user_id is None:
            raise HTTPException(status_code=401)
    except JWTError:
        raise HTTPException(status_code=401)

    user = get_user(user_id)
    if user is None:
        raise HTTPException(status_code=401)

    return user
```

**Redisë¥¼ ì‚¬ìš©í•œ API ì†ë„ ì œí•œ:**

```python
from fastapi import HTTPException
import time

class RateLimiter:
    def __init__(self, redis_client, requests: int, window: int):
        self.redis = redis_client
        self.requests = requests
        self.window = window

    async def check_rate_limit(self, user_id: str):
        key = f"rate_limit:{user_id}"
        current = await self.redis.get(key)

        if current is None:
            # ì²« ìš”ì²­
            await self.redis.setex(key, self.window, 1)
            return True

        if int(current) >= self.requests:
            raise HTTPException(
                status_code=429,
                detail=f"ì†ë„ ì œí•œ ì´ˆê³¼. {self.window}ì´ˆ í›„ì— ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
            )

        await self.redis.incr(key)
        return True

rate_limiter = RateLimiter(redis_client, requests=100, window=60)

@app.post("/api/analyze")
async def analyze(user: User = Depends(get_current_user)):
    await rate_limiter.check_rate_limit(user.id)
    # ... ìš”ì²­ ì²˜ë¦¬
```

#### ë¹„ìš© ìµœì í™”

**LLM API ë¹„ìš© ê´€ë¦¬:**

```python
class CostOptimizedLLMService:
    def __init__(self):
        self.client = anthropic.Anthropic(api_key=settings.ANTHROPIC_API_KEY)
        self.cache = RedisCache()

        # ë¹„ìš© ì¶”ì 
        self.cost_tracker = {
            'input_tokens': 0,
            'output_tokens': 0,
            'total_cost': 0.0
        }

    async def analyze(self, text: str, target_language: str) -> Dict:
        # 1. ë¨¼ì € ìºì‹œ í™•ì¸
        cache_key = f"{hash(text)}:{target_language}"
        if cached := await self.cache.get(cache_key):
            return cached

        # 2. í”„ë¡¬í”„íŠ¸ ì••ì¶• ì‚¬ìš©
        compressed_prompt = self._compress_prompt(text)

        # 3. ë³µì¡ë„ì— ë”°ë¼ ëª¨ë¸ ì¡°ì •
        model = self._select_model(text)

        # 4. API í˜¸ì¶œ
        response = self.client.messages.create(
            model=model,
            max_tokens=self._calculate_max_tokens(text),
            messages=[{"role": "user", "content": compressed_prompt}]
        )

        # 5. ì‚¬ìš©ëŸ‰ ì¶”ì 
        self._track_usage(response.usage)

        # 6. ê²°ê³¼ ìºì‹±
        await self.cache.setex(cache_key, 3600, response)

        return response

    def _select_model(self, text: str) -> str:
        """ì‘ì—… ë³µì¡ë„ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ"""
        word_count = len(text.split())

        if word_count < 100:
            return "claude-3-haiku-20240307"  # ë” ì €ë ´í•˜ê³  ë¹ ë¦„
        elif word_count < 500:
            return "claude-3-5-sonnet-20241022"  # ê· í˜•ì¡íŒ
        else:
            return "claude-3-5-sonnet-20241022"  # ìµœê³  í’ˆì§ˆ

    def _compress_prompt(self, text: str) -> str:
        """ì¤‘ë³µ ì •ë³´ ì œê±°"""
        # ê³¼ë„í•œ ê³µë°± ì œê±°
        compressed = ' '.join(text.split())

        # ë„ˆë¬´ ê¸¸ë©´ ì¶”ì¶œì  ìš”ì•½ ì‚¬ìš©
        if len(compressed) > 10000:
            # ì²˜ìŒê³¼ ë§ˆì§€ë§‰ ë‹¨ë½ ìœ ì§€, ì¤‘ê°„ ìš”ì•½
            sentences = sent_tokenize(compressed)
            if len(sentences) > 20:
                return ' '.join(sentences[:5] + ['...'] + sentences[-5:])

        return compressed

    def _track_usage(self, usage):
        """í† í° ì‚¬ìš©ëŸ‰ ë° ì˜ˆìƒ ë¹„ìš© ì¶”ì """
        # Claude ê°€ê²© (ì˜ˆì‹œ)
        INPUT_COST_PER_1K = 0.003
        OUTPUT_COST_PER_1K = 0.015

        input_cost = (usage.input_tokens / 1000) * INPUT_COST_PER_1K
        output_cost = (usage.output_tokens / 1000) * OUTPUT_COST_PER_1K

        self.cost_tracker['input_tokens'] += usage.input_tokens
        self.cost_tracker['output_tokens'] += usage.output_tokens
        self.cost_tracker['total_cost'] += input_cost + output_cost

        # ë¹„ìš©ì´ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ë©´ ì•Œë¦¼
        if self.cost_tracker['total_cost'] > 100:  # $100
            send_alert("LLM ë¹„ìš©ì´ $100ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤")
```

### ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸

**ì»¤ìŠ¤í…€ ë¶„ì„ ëŒ€ì‹œë³´ë“œ:**

```python
# analytics.py
from sqlalchemy import func, and_
from datetime import datetime, timedelta

class AnalyticsService:
    def __init__(self, db_session):
        self.db = db_session

    async def get_usage_stats(self, days: int = 30) -> Dict:
        """ì¢…í•©ì ì¸ ì‚¬ìš© í†µê³„ ì–»ê¸°"""
        start_date = datetime.now() - timedelta(days=days)

        # ë¬¸ì„œ ì²˜ë¦¬ í†µê³„
        total_docs = await self.db.query(func.count(Document.id))\
            .filter(Document.created_at >= start_date)\
            .scalar()

        # ë¬¸ì„œ ìœ í˜•ë³„
        by_type = await self.db.query(
            Document.document_type,
            func.count(Document.id)
        ).filter(
            Document.created_at >= start_date
        ).group_by(Document.document_type).all()

        # ì–¸ì–´ ìŒ
        language_pairs = await self.db.query(
            Document.source_language,
            Document.target_language,
            func.count(Document.id)
        ).filter(
            Document.created_at >= start_date
        ).group_by(
            Document.source_language,
            Document.target_language
        ).all()

        # í‰ê·  ì‹ ë¢°ë„ ì ìˆ˜
        avg_confidence = await self.db.query(
            func.avg(Document.confidence_score)
        ).filter(
            Document.created_at >= start_date
        ).scalar()

        # ì‚¬ìš©ì ì°¸ì—¬ë„
        active_users = await self.db.query(
            func.count(func.distinct(Document.user_id))
        ).filter(
            Document.created_at >= start_date
        ).scalar()

        # í”¼í¬ ì‚¬ìš© ì‹œê°„
        hourly_usage = await self.db.query(
            func.extract('hour', Document.created_at).label('hour'),
            func.count(Document.id)
        ).filter(
            Document.created_at >= start_date
        ).group_by('hour').all()

        return {
            "total_documents": total_docs,
            "by_document_type": dict(by_type),
            "language_pairs": [
                {"from": src, "to": tgt, "count": cnt}
                for src, tgt, cnt in language_pairs
            ],
            "average_confidence": float(avg_confidence or 0),
            "active_users": active_users,
            "hourly_distribution": dict(hourly_usage)
        }
```

---

## ê²°ë¡ 

ì´ ê°€ì´ë“œëŠ” Universal Document Translator í”„ë¡œì íŠ¸ì˜ ê¸°ë³¸ ì´í•´ë¶€í„° ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­ê¹Œì§€ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ì´:

- **í•™ìƒ**ìœ¼ë¡œ OCR ë° AIì— ëŒ€í•´ ë°°ìš°ëŠ” ê²½ìš°
- **ê°œë°œì**ë¡œ ìœ ì‚¬í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ëŠ” ê²½ìš°
- **ì „ë¬¸ê°€**ë¡œ í”„ë¡œë•ì…˜ ì‹œìŠ¤í…œì„ ìµœì í™”í•˜ëŠ” ê²½ìš°

ì´ í”„ë¡œì íŠ¸ëŠ” ìµœì²¨ë‹¨ AI í†µí•©ì„ ì‚¬ìš©í•œ í˜„ëŒ€ í’€ìŠ¤íƒ ê°œë°œì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

### í•µì‹¬ ìš”ì 

1. **OCR ê¸°ìˆ **: ì»´í“¨í„°ê°€ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ë°©ë²• ì´í•´
2. **LLM í†µí•©**: ë²ˆì—­ ë° ë¶„ì„ì„ ìœ„í•œ AI í™œìš©
3. **í’€ìŠ¤íƒ ê°œë°œ**: í™•ì¥ ê°€ëŠ¥í•œ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶•
4. **ëª¨ë²” ì‚¬ë¡€**: ë³´ì•ˆ, ì„±ëŠ¥ ë° ì½”ë“œ í’ˆì§ˆ
5. **í”„ë¡œë•ì…˜ ì¤€ë¹„**: ë°°í¬, ëª¨ë‹ˆí„°ë§ ë° ìœ ì§€ë³´ìˆ˜

### ë‹¤ìŒ ë‹¨ê³„

- https://github.com/dddoing/ocrproject ì—ì„œ ì½”ë“œë² ì´ìŠ¤ íƒìƒ‰
- ìì‹ ë§Œì˜ ê¸°ëŠ¥ êµ¬ì¶• ì‹œë„
- í”„ë¡œì íŠ¸ì— ê¸°ì—¬
- í”„ë¡œë•ì…˜ì— ë°°í¬

**ì¦ê±°ìš´ ì½”ë”© ë˜ì„¸ìš”!**
